{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5b906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import helpers\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "253c73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load train data \n",
    "y_tr, tx_tr, ids_tr = helpers.load_csv_data('./data/train.csv', sub_sample = False)\n",
    "y_te, tx_te, ids_te = helpers.load_csv_data('./data/test.csv', sub_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832d8284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/0nrx2cz904vd8vf6qt5jttqm0000gn/T/ipykernel_64745/3183830410.py:14: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w,_,_,_ = np.linalg.lstsq(left,right)\n",
      "/var/folders/7t/0nrx2cz904vd8vf6qt5jttqm0000gn/T/ipykernel_64745/1628065401.py:20: RuntimeWarning: Mean of empty slice.\n",
      "  MSE = 1/2*x.mean()\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "## implementation du classifier SVM_UltraBoost\n",
    "## son train prend un peu de temps, il se fait en cinq étapes, avec un print 'ok' \n",
    "## à chacune d'entres elles\n",
    "## faire tourner toutes les fonctions plus bas avant de faire tourner cette partie\n",
    "## le préprocessing se fait en interne, donc pas besoin de le faire ici\n",
    "\n",
    "## N.B. plus on augmente le paramètre size, plus le temps de train est long\n",
    "## il s'agit du nombre de points sur lequel est entrainé chaque \"sous-classifier\"\n",
    "\n",
    "\n",
    "classifier = Boost2(size=50000)\n",
    "classifier.fit(tx_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11cbb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813968\n"
     ]
    }
   ],
   "source": [
    "pred = classifier.predict(tx_tr)\n",
    "print(accuracy(y_tr, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "509773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(pred)\n",
    "export_to_csv(np.reshape(pred, (length, 1)), np.reshape(ids_te, (length, 1)), 'submissions2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "591901c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(ypred,Id_test, text):\n",
    "    output = np.concatenate((Id_test, ypred),axis = 1)\n",
    "    np.savetxt(text, output, delimiter=\",\",fmt= \"%i\", header = \"Id,Prediction\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31d3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "## every useful classes : \n",
    "## SVM_UltraBoost\n",
    "## SVM_boosted3\n",
    "## SVM_boosted2\n",
    "## SVM_boosted\n",
    "## SVM\n",
    "\n",
    "class Boost2:\n",
    "    \n",
    "    def __init__(self, size = 20000):\n",
    "        self.size = size\n",
    "        self.Boost1 = Boost(size = 20000)\n",
    "        self.Boost2 = Boost(size = 20000)\n",
    "        self.Boost3 = Boost(size = 20000)\n",
    "        self.Boost4 = Boost(size = 20000)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all subsets\n",
    "        S1 = (X[:, 22]==0.)\n",
    "        S2 = (X[:, 22]==1.)\n",
    "        S3 = (X[:, 22]==2.)\n",
    "        S4 = (X[:, 22]==3.)\n",
    "        \n",
    "        \n",
    "        ## we create the classifiers\n",
    "        \n",
    "        self.Boost1.fit(X[S1], Y[S1])\n",
    "        self.Boost2.fit(X[S2], Y[S2])\n",
    "        self.Boost3.fit(X[S3], Y[S3])\n",
    "        self.Boost4.fit(X[S4], Y[S4])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all the subsets\n",
    "        S1 = (X[:, 22]==0.)\n",
    "        S2 = (X[:, 22]==1.)\n",
    "        S3 = (X[:, 22]==2.)\n",
    "        S4 = (X[:, 22]==3.)\n",
    "        \n",
    "        ## we make the predictions\n",
    "        Y = np.ones(X.shape[0])\n",
    "        Y[S1] = self.Boost1.predict(X[S1])\n",
    "        Y[S2] = self.Boost2.predict(X[S2])\n",
    "        Y[S3] = self.Boost3.predict(X[S3])\n",
    "        Y[S4] = self.Boost4.predict(X[S4])\n",
    "        \n",
    "        return Y\n",
    "\n",
    "class Boost:\n",
    "    \n",
    "    def __init__(self, size = 20000):\n",
    "        self.size = size\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.w4 = None\n",
    "        self.w6 = None\n",
    "        self.w7 = None\n",
    "        self.w8 = None\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all subsets\n",
    "        S1 = (X[:, 0]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]>118.)\n",
    "        S2 = (X[:, 0]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]<=118.)\n",
    "        S4 = (X[:, 0]!=-999.)&(X[:, 4]==-999.)&(X[:,0]>118.)\n",
    "        S6 = (X[:, 0]!=-999.)&(X[:, 4]==-999.)&(X[:,0]<=118.)\n",
    "        S7 = (X[:, 0]==-999.)&(X[:, 4]!=-999.)\n",
    "        S8 = (X[:, 0]==-999.)&(X[:, 4]==-999.)\n",
    "        \n",
    "        ## we create the classifiers\n",
    "        \n",
    "        self.w1, _ = least_squares(Y[S1][:self.size], X[S1][:self.size])\n",
    "        self.w2, _ = least_squares(Y[S2][:self.size], X[S2][:self.size])\n",
    "        self.w4, _ = least_squares(Y[S4][:self.size], X[S4][:self.size])\n",
    "        self.w6, _ = least_squares(Y[S6][:self.size], X[S6][:self.size])\n",
    "        self.w7, _ = least_squares(Y[S7][:self.size], X[S7][:self.size])\n",
    "        self.w8, _ = least_squares(Y[S8][:self.size], X[S8][:self.size])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all the subsets\n",
    "        S1 = (X[:, 0]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]>118.)\n",
    "        S2 = (X[:, 0]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]<=118.)\n",
    "        S4 = (X[:, 0]!=-999.)&(X[:, 4]==-999.)&(X[:,0]>118.)\n",
    "        S6 = (X[:, 0]!=-999.)&(X[:, 4]==-999.)&(X[:,0]<=118.)\n",
    "        S7 = (X[:, 0]==-999.)&(X[:, 4]!=-999.)\n",
    "        S8 = (X[:, 0]==-999.)&(X[:, 4]==-999.)\n",
    "        \n",
    "        ## we make the predictions\n",
    "        Y = np.ones(X.shape[0])\n",
    "        Y[S1] = np.sign(X[S1].dot(self.w1))\n",
    "        Y[S2] = np.sign(X[S2].dot(self.w2))\n",
    "        Y[S6] = np.sign(X[S6].dot(self.w6))\n",
    "        Y[S4] = np.sign(X[S4].dot(self.w4))\n",
    "        Y[S7] = np.sign(X[S7].dot(self.w7))\n",
    "        Y[S8] = np.sign(X[S8].dot(self.w8))\n",
    "        \n",
    "        \n",
    "        return Y\n",
    "\n",
    "class SVM_UltraBoost:\n",
    "    \n",
    "    ## train 5 identical SVM_boosted3 models, on different subsets of the train data\n",
    "    ## then return the most occurent prediction of the 5 classifiers  \n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, size = 500):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.size = size\n",
    "        self.SVM1 = SVM_boosted3(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM2 = SVM_boosted3(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM3 = SVM_boosted3(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM4 = SVM_boosted3(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM5 = SVM_boosted3(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        length = X.shape[0]\n",
    "        intervals = np.linspace(0, length, 6).astype(int)\n",
    "        \n",
    "        ## we create the classifiers\n",
    "        self.SVM1.fit(X[intervals[0]:intervals[1]], Y[intervals[0]:intervals[1]].flatten())\n",
    "        print('ok1')\n",
    "        self.SVM2.fit(X[intervals[1]:intervals[2]], Y[intervals[1]:intervals[2]].flatten())\n",
    "        print('ok2')\n",
    "        self.SVM3.fit(X[intervals[2]:intervals[3]], Y[intervals[2]:intervals[3]].flatten())\n",
    "        print('ok3')\n",
    "        self.SVM4.fit(X[intervals[3]:intervals[4]], Y[intervals[3]:intervals[4]].flatten())\n",
    "        print('ok4')\n",
    "        self.SVM5.fit(X[intervals[4]:intervals[5]], Y[intervals[4]:intervals[5]].flatten())\n",
    "        print('ok5')\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we make the predictions\n",
    "        Y1 = self.SVM1.predict(X)\n",
    "        Y2 = self.SVM1.predict(X)\n",
    "        Y3 = self.SVM1.predict(X)\n",
    "        Y4 = self.SVM1.predict(X)\n",
    "        Y5 = self.SVM1.predict(X)\n",
    "        \n",
    "        Y = (Y1 + Y2 + Y3 + Y4 + Y5)\n",
    "        Y[Y >= 0.] =  1.\n",
    "        Y[Y <  0.] = -1.\n",
    "        \n",
    "        return Y\n",
    "\n",
    "class SVM_boosted3:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, size = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.size = size\n",
    "        self.SVM1 = SVM_boosted2(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM2 = SVM_boosted2(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all subsets\n",
    "        S1 = (X[:, 27] >= 0.)\n",
    "        S2 = (X[:, 27] <  0.)\n",
    "        \n",
    "        ## we create the classifiers\n",
    "        self.SVM1.fit(X[S1], Y[S1].flatten())\n",
    "        self.SVM2.fit(X[S2], Y[S2].flatten())\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all the subsets\n",
    "        S1 = (X[:, 27] >= 0.)\n",
    "        S2 = (X[:, 27] <  0.)\n",
    "        \n",
    "        ## we make the predictions\n",
    "        Y = np.ones(X.shape[0])\n",
    "        Y[S1] = self.SVM1.predict(X[S1])\n",
    "        Y[S2] = self.SVM2.predict(X[S2])\n",
    "        \n",
    "        return Y\n",
    "\n",
    "class SVM_boosted2:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, size = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.size = size\n",
    "        self.SVM1 = SVM_boosted(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM2 = SVM_boosted(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM3 = SVM_boosted(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        self.SVM4 = SVM_boosted(self.lr, self.lambda_param, self.n_iters, self.size)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all subsets\n",
    "        S1 = (X[:, 22]==0.)\n",
    "        S2 = (X[:, 22]==1.)\n",
    "        S3 = (X[:, 22]==2.)\n",
    "        S4 = (X[:, 22]==3.)\n",
    "        \n",
    "        \n",
    "        ## we create the classifiers\n",
    "        self.SVM1.fit(X[S1], Y[S1].flatten())\n",
    "        self.SVM2.fit(X[S2], Y[S2].flatten())\n",
    "        self.SVM3.fit(X[S3], Y[S3].flatten())\n",
    "        self.SVM4.fit(X[S4], Y[S4].flatten())\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all the subsets\n",
    "        S1 = (X[:, 22]==0.)\n",
    "        S2 = (X[:, 22]==1.)\n",
    "        S3 = (X[:, 22]==2.)\n",
    "        S4 = (X[:, 22]==3.)\n",
    "        \n",
    "        ## we make the predictions\n",
    "        Y = np.ones(X.shape[0])\n",
    "        Y[S1] = self.SVM1.predict(X[S1])\n",
    "        Y[S2] = self.SVM2.predict(X[S2])\n",
    "        Y[S3] = self.SVM3.predict(X[S3])\n",
    "        Y[S4] = self.SVM4.predict(X[S4])\n",
    "        \n",
    "        \n",
    "        return Y\n",
    "\n",
    "class SVM_boosted:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, size = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.size = size\n",
    "        self.SVM1 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM2 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM3 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM4 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM5 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM6 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM7 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM8 = SVM(self.lr, self.lambda_param, self.n_iters)\n",
    "        self.SVM9 = SVM(self.lr, self.lambda_param, self.n_iters) \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all subsets\n",
    "        S1 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]>118.)\n",
    "        S2 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]<=118.)\n",
    "        S3 = (X[:, 0]!=-999.)&(X[:, 23]==-999.)&(X[:, 4]==-999.)&(X[:,0]>118.)\n",
    "        S4 = (X[:, 0]!=-999.)&(X[:, 23]==-999.)&(X[:, 4]==-999.)&(X[:,0]<=118.)\n",
    "        S5 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]==-999.)&(X[:,0]>118.)\n",
    "        S6 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]==-999.)&(X[:,0]<=118.)\n",
    "        S7 = (X[:, 0]==-999.)&(X[:, 23]!=-999.)&(X[:, 4]!=-999.)\n",
    "        S8 = (X[:, 0]==-999.)&(X[:, 23]!=-999.)&(X[:, 4]==-999.)\n",
    "        S9 = (X[:, 0]==-999.)&(X[:, 23]==-999.)&(X[:, 4]==-999.)\n",
    "        \n",
    "        ## we create the classifiers\n",
    "        self.SVM1.fit(remove_unique(standardize(treat_data(X[S1][:self.size]))), Y[S1][:self.size].flatten())\n",
    "        self.SVM2.fit(remove_unique(standardize(treat_data(X[S2][:self.size]))), Y[S2][:self.size].flatten())\n",
    "        self.SVM3.fit(remove_unique(standardize(treat_data(X[S3][:self.size]))), Y[S3][:self.size].flatten())\n",
    "        self.SVM4.fit(remove_unique(standardize(treat_data(X[S4][:self.size]))), Y[S4][:self.size].flatten())\n",
    "        self.SVM5.fit(remove_unique(standardize(treat_data(X[S5][:self.size]))), Y[S5][:self.size].flatten())\n",
    "        self.SVM6.fit(remove_unique(standardize(treat_data(X[S6][:self.size]))), Y[S6][:self.size].flatten())\n",
    "        self.SVM7.fit(remove_unique(standardize(treat_data(X[S7][:self.size]))), Y[S7][:self.size].flatten())\n",
    "        self.SVM8.fit(remove_unique(standardize(treat_data(X[S8][:self.size]))), Y[S8][:self.size].flatten())\n",
    "        self.SVM9.fit(remove_unique(standardize(treat_data(X[S9][:self.size]))), Y[S9][:self.size].flatten())\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        \n",
    "        ## we prepare all the subsets\n",
    "        S1 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]>118.)\n",
    "        S2 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]!=-999.)&(X[:,0]<=118.)\n",
    "        S3 = (X[:, 0]!=-999.)&(X[:, 23]==-999.)&(X[:, 4]==-999.)&(X[:,0]>118.)\n",
    "        S4 = (X[:, 0]!=-999.)&(X[:, 23]==-999.)&(X[:, 4]==-999.)&(X[:,0]<=118.)\n",
    "        S5 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]==-999.)&(X[:,0]>118.)\n",
    "        S6 = (X[:, 0]!=-999.)&(X[:, 23]!=-999.)&(X[:, 4]==-999.)&(X[:,0]<=118.)\n",
    "        S7 = (X[:, 0]==-999.)&(X[:, 23]!=-999.)&(X[:, 4]!=-999.)\n",
    "        S8 = (X[:, 0]==-999.)&(X[:, 23]!=-999.)&(X[:, 4]==-999.)\n",
    "        S9 = (X[:, 0]==-999.)&(X[:, 23]==-999.)&(X[:, 4]==-999.)\n",
    "        \n",
    "        ## we make the predictions\n",
    "        Y = np.ones(X.shape[0])\n",
    "        Y[S1] = self.SVM1.predict(remove_unique(standardize(treat_data(X[S1]))))\n",
    "        Y[S2] = self.SVM2.predict(remove_unique(standardize(treat_data(X[S2]))))\n",
    "        Y[S3] = self.SVM3.predict(remove_unique(standardize(treat_data(X[S3]))))\n",
    "        Y[S4] = self.SVM4.predict(remove_unique(standardize(treat_data(X[S4]))))\n",
    "        Y[S5] = self.SVM5.predict(remove_unique(standardize(treat_data(X[S5]))))\n",
    "        Y[S6] = self.SVM6.predict(remove_unique(standardize(treat_data(X[S6]))))\n",
    "        Y[S7] = self.SVM7.predict(remove_unique(standardize(treat_data(X[S7]))))\n",
    "        Y[S8] = self.SVM8.predict(remove_unique(standardize(treat_data(X[S8]))))\n",
    "        Y[S9] = self.SVM9.predict(remove_unique(standardize(treat_data(X[S9]))))\n",
    "        \n",
    "        return Y\n",
    "        \n",
    "    def standardize(x):\n",
    "        centered_data = x - np.mean(x, axis=0)\n",
    "        std = np.std(centered_data, axis=0)\n",
    "        std[std==0.] = 1\n",
    "        std_data = centered_data / std\n",
    "        return std_data\n",
    "    \n",
    "    def treat_data(x, mean = True):\n",
    "    \n",
    "        '''\n",
    "        get an imput array of features (columns) and replace all -999. values \n",
    "        by the mean value of their column.\n",
    "    \n",
    "        '''\n",
    "    \n",
    "        xx = x.copy()\n",
    "        for ind, column in enumerate(xx.T[:]):\n",
    "            if mean:\n",
    "                if np.isnan(column[column!=-999.].mean())==False:\n",
    "                    column[column==-999.] = column[column!=-999.].mean()\n",
    "                else: \n",
    "                    column[column==-999.] = 0\n",
    "            \n",
    "            else:\n",
    "                return\n",
    "            ##rien pour l'instant\n",
    "            xx.T[ind] = column\n",
    "        \n",
    "        return xx\n",
    "    \n",
    "    def remove_unique(xx):\n",
    "        xxx = np.array(xx.copy())\n",
    "        liste = []\n",
    "        for ind, column in enumerate(xx.T[:]):\n",
    "            if np.std(column)==0.:\n",
    "                liste.append(ind)\n",
    "        xxx = np.delete(xxx, obj = liste  , axis = 1) \n",
    "        return xxx\n",
    "    \n",
    "\n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        \"\"\"Computes the optimal model parameters w and b for the soft vector model \n",
    "        Args:\n",
    "        y: shape=(N, 1)\n",
    "        X: shape=(N,M)\n",
    "        w: shape=(M, 1). The vector of model parameters.\n",
    "        lambda_ : Ridge regularization parameter\n",
    "        Returns:\n",
    "        Value of optimal w  \n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predictions with new data on SVM \n",
    "        \"\"\"\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "    \n",
    "def confusion_matrix(y,ypred):\n",
    "    \"\"\" Evaluates the confusion matrix for classification \n",
    "    Args:\n",
    "        y: shape=(N, 1) variable to be predicted\n",
    "        ypred: shape=(N, 1) variable predicted by the model\n",
    "    Returns:\n",
    "         confusion matrix\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    n=len(y)\n",
    "    for actual_value, predicted_value in zip(y, ypred):\n",
    "        if predicted_value == actual_value:\n",
    "            if predicted_value == 1:\n",
    "                tp += 1\n",
    "            else: \n",
    "                tn += 1\n",
    "        else: \n",
    "            if predicted_value == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "                \n",
    "    confusion_matrix = [[tn/n, fp/n], [fn/n, tp/n]]\n",
    "    confusion_matrix = np.array(confusion_matrix)\n",
    "    return confusion_matrix\n",
    "\n",
    "def accuracy(y,ypred): \n",
    "    matrix = confusion_matrix(y,ypred)\n",
    "    return matrix[0,0]+ matrix[1,1]\n",
    "\n",
    "def standardize(x):\n",
    "        centered_data = x - np.mean(x, axis=0)\n",
    "        \n",
    "        std = np.std(centered_data, axis=0)\n",
    "        std[std==0.] = 1\n",
    "        \n",
    "        std_data = centered_data / std\n",
    "        \n",
    "    \n",
    "        return std_data\n",
    "    \n",
    "def treat_data(x, mean = True):\n",
    "    \n",
    "    '''\n",
    "    get an imput array of features (columns) and replace all -999. values \n",
    "    by the mean value of their column.\n",
    "    '''\n",
    "    \n",
    "    xx = x.copy()\n",
    "    for ind, column in enumerate(xx.T[:]):\n",
    "        if mean:\n",
    "            if np.isnan(column[column!=-999.].mean())==False:\n",
    "                column[column==-999.] = column[column!=-999.].mean()\n",
    "            else: \n",
    "                column[column==-999.] = 0\n",
    "            \n",
    "        else:\n",
    "            return\n",
    "            ##rien pour l'instant\n",
    "        xx.T[ind] = column\n",
    "        \n",
    "    return xx\n",
    "\n",
    "def remove_unique(xx):\n",
    "    xxx = np.array(xx.copy())\n",
    "    liste = []\n",
    "    for ind, column in enumerate(xx.T[:]):\n",
    "        if np.std(column)==0.:\n",
    "            liste.append(ind)\n",
    "    xxx = np.delete(xxx, obj = liste  , axis = 1) \n",
    "    return xxx\n",
    "\n",
    "\n",
    "def log_right_skewed(x):\n",
    "    eps = 1e-6\n",
    "    xx = x.copy()\n",
    "    #ids = np.array([0, 1, 2, 3, 5, 8, 9, 10, 13, 16, 23, 26, 29])\n",
    "    idx_3 = np.array([0, 1, 2, 5, 8, 9, 10, 13, 19, 23, 26, 29])\n",
    "    for i in idx_3:\n",
    "        xxx = xx[xx[:,i]!=-999.] \n",
    "        minimum = xxx[:, i].min()\n",
    "        xx[xx[:,i]!=-999.][:, i] = np.log(1+xxx[:, i]+eps-minimum)\n",
    "    return xx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159840da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \n",
    "    \"\"\"\n",
    "    compute the least square regression using normal equation\n",
    "    inputs:\n",
    "        y: shape=(N, 1)\n",
    "        tx: shape=(N,M)\n",
    "    outputs:\n",
    "        w: shape=(M,1)\n",
    "        MSE: float \n",
    "    \"\"\"\n",
    "    left = tx.T@tx\n",
    "    right = tx.T@y\n",
    "    w,_,_,_ = np.linalg.lstsq(left,right)\n",
    "    loss = compute_mse_loss(y,tx,w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd9a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # MSE : \n",
    "    \n",
    "    y_pred = tx@w\n",
    "    x = (y_pred-y)**2\n",
    "    MSE = 1/2*x.mean()\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc542fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
